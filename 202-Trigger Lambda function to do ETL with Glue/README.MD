## Trigger Lambda function to do ETL with Glue

After you upload data into S3, then trigger a Lambda function to do ETL which is a type of data integration that refers to the three steps (Extrect, Transform and Load) used to blend data from multiple sources, here we will use [AWS Glue](https://aws.amazon.com/tw/glue/) to do it.


### Setup data catalog in AWS Glue

Create database, tables, crawlers, jobs in Glue<br><br>
* 	On the **Services** menu, click **AWS Glue**.<br><br>
* 	In the console, choose **Add database**. In the **Database name**, type **my-data**, and choose **Create**.<br><br>
* 	Choose **Crawlers** in the navigation pane, choose **Add crawler**. Add type Crawler name **data-crawler**, and choose **Next**.<br><br>
* 	On the **Add a data store** page, choose **S3** as data store.<br><br>
* 	Select **Specified path in my account**.<br><br>
* 	Select the bucket that you create first (**“yourname-dataset”****), and choose **Next**.<br><br>
![glue1.png](/images/glue1.png)<br>  
* 	On **Add another data store** page, choose **No**, and choose **Next**.<br><br>
* 	Select **Choose an existing IAM role**, and choose the role **AWSGlueServiceRoleDefault** you just created in the drop-down list, and choose **Next**.<br><br>
* 	For **Frequency**, choose **Run on demand**, and choose **Next**.<br><br>
* 	For **Database**, choose **my-data**, and choose **Next**.<br><br>
* 	Review the steps, and choose **Finish**.<br><br>
* 	The crawler is ready to run. Choose **Run it now**.<br><br>
* 	When the crawler has finished, two table has been added. Choose **Tables** in the left navigation pane, and then choose **usvideos_csv** to confirmed.<br><br>
![glue2.png](/images/glue2.png)<br>  
![glue3.png](/images/glue3.png)<br>  
Now you finish the crawler setting, and going to create a job to transform data type<br><br>
 
* 	In the navigation pane, under **ETL**, choose **Jobs**, and then choose **Add job**.<br><br>
* 	On the Job properties, enter the following details:<br>
**Name: data-csv-parquet**<br>
**IAM role:** choose **AWSGlueServiceRoleDefault**<br><br>
* 	For **This job runs**, select **A proposed script generated by AWS Glue**.<br><br>
* 	Choose **Next**.<br><br>
* 	Choose **usvideos_csv** as the data source, and choose **Next**.<br><br>
* 	Choose **Create tables in your data target**.<br><br>
*	For Data store, choose Amazon S3, and choose **Parquet** as the format.<br><br>
*	For **Target path**, select S3 bucket with **“yourname-etl-result”** that you created before to store the results.<br><br>
![glue4.png](/images/glue4.png)<br>  
* 	Verify the schema mapping, and choose **Next** and click **Finish**.<br><br>
![glue5.png](/images/glue5.png)<br>  
* 	View the job. This screen provides a complete view of the job and allows you to edit, click **Save**, and choose **Run job**. This steps may be waiting around 10 minutes.<br><br>
![glue6.png](/images/glue6.png)<br> 
Job running screen<br><br>
* 	When job finished, go to your S3 bucket **“yourname-etl-result”** ensure there is a parquet file that means your job succeed.<br><br>
![glue7.png](/images/glue7.png)<br>  
 
Now you need to add another parquet table and crawler<br><br>
* 	When the job has finished, add a new table for the Parquet data using a crawler.<br><br>
* 	In the navigation pane, choose **Add crawler**. Add type Crawler name **“parquet-crawler1”** and choose **Next**.<br><br>
* 	Choose **S3** as the **Data store**.<br><br>
* 	Include path choose your S3 bucket **“yourname-etl-result”** to store data.<br><br><br><br>
* 	Choose **Next**.<br><br>
*	On **Add another data store** page, choose **No**, and choose **Next**.<br><br>
* 	Select **Choose an existing IAM role**, and choose the role <br><br>
**AWSGlueServiceRoleDefault** you just created in the drop-down list, and choose Next.<br><br>
* 	For **Frequency**, choose **Run on demand**, and choose **Next**.<br><br>
* 	For **Database****, choose **my-data**, and choose **Next**.<br><br>
* 	Review the steps, and choose **Finish**.<br><br>
* 	The crawler is ready to run. Choose **Run it now**.<br><br>
* 	After the crawler has finished, there is a new table in the **my-data** database:<br><br>
![glue8.png](/images/glue8.png)<br>  
That means you finish the ETL process of csv to parquet format **(USvideos.csv)**<br><br>

In order to analyze the channel category, we need to perform another job in Glue data catalog which contains category data (**US-category-id.json**)<br><br>
* 	In the navigation pane, under **ETL**, choose **Jobs**, and then choose **Add job**.<br><br>
* 	On the Job properties, enter the following details:<br>
**Name: data-json-parquet**<br>
**IAM role:** choose **AWSGlueServiceRoleDefault**<br><br>
* 	For **This job runs**, select **A proposed script generated by AWS Glue**.<br><br>
* 	Choose **Next**.<br><br>
*	Choose **us_category_id_json** as the data source, and choose **Next**.<br><br>
* 	Choose **Create tables in your data target**.<br><br>
*	For Data store, choose Amazon S3, and choose **Parquet** as the format.<br><br>
* 	For **Target path**, select S3 bucket with **“yourname-etl-result2”** that you created before to store the results.<br><br>
![glue9.png](/images/glue9.png)<br>  
* 	Verify the schema mapping, and choose **Next** and click **Finish**.<br><br>
![glue10.png](/images/glue10.png)<br>  
* 	View the job. This screen provides a complete view of the job and allows you to edit, click **Save**, and choose **Run job**. This steps may be waiting around 10 minutes.<br><br>
![glue11.png](/images/glue11.png)<br> 
Job running screen<br><br>
* 	When job finished, go to your S3 bucket **“yourname-etl-result2”** ensure there is a parquet file that means your job succeed.<br><br>
![glue12.png](/images/glue12.png)<br>  
 
Now you need to add another parquet table and crawler<br><br>
* 	When the job has finished, add a new table for the Parquet data using a crawler.<br><br>
* 	In the navigation pane, choose **Add crawler**. Add type Crawler name **“parquet-data2”** and choose **Next**.<br><br>
* 	Choose **S3** as the **Data store**.<br><br>
* 	Include path choose your S3 bucket **“yourname-etl-result2”** to store data.<br><br>
* 	Choose **Next**.<br><br>
* 	On **Add another data store** page, choose **No**, and choose **Next**.<br><br>
* 	Select **Choose an existing IAM role**, and choose the role<br> 
**AWSGlueServiceRoleDefault** you just created in the drop-down list, and choose **Next**.<br><br>
* 	For **Frequency**, choose **Run on demand**, and choose **Next**.<br><br>
* 	For **Database**, choose **my-data**, and choose **Next**.<br><br>
*	Review the steps, and choose **Finish**.<br><br>
* 	The crawler is ready to run. Choose **Run it now**.<br><br>
* 	After the crawler has finished, there is a new table in the **my-data** database:<br><br>
![glue13.png](/images/glue13.png)<br>  
Congratulations! You now have learned how to:<br><br>
•	Processing ETL job manually using AWS Glue and Amazon S3.<br><br>
•	Crawler your data to Amazon S3 by AWS Glue.<br><br>
Next step you will learn how to trigger ETL job with Lambda function automatically<br>



### Create a Lambda function in order to automate ETL job with glue (automated ETL job)

* 	On the **Services** menu, click **Lambda**.<br><br>
* 	Click **Create function**.<br><br>
* 	Choose **Author from scratch**.<br><br>
* 	Enter function Name **ETL-auto**.<br><br>
* 	Select **python 3.6** in **Runtime** blank.<br><br>
* 	Select **Choose an existing role** in Role blank and choose **LambdaAutoETL** as **Existing role**.<br><br>
![lambda_glue1.png](/images/lambda_glue1.png)<br>   
* 	Click **Create function**.<br><br>
* 	In **configuration**, click **S3** below **Add triggers** to add trigger for **ETL-auto** function<br><br>
and drop down to **Configure triggers** part, select bucket **“yourname-dataset”** as Bucket, select **PUT** as **Event type**. Remember to check **Enable trigger** box then you click **Add**.<br><br>
![lambda_glue2.png](/images/lambda_glue2.png)<br>
![lambda_glue3.png](/images/lambda_glue3.png)<br>  
* 	Click **ETL-auto** blank in **Designer** and replace original code that existing in **Function code** editor with below code<br><br>
 
         import boto3
         import json

         glue = boto3.client('glue')

         def lambda_handler(event, context):
             # TODO implement
             response = glue.start_job_run(
                 JobName = 'data-csv-parquet',
                 Arguments = {})
             return response
 
![lambda_glue4.png](/images/lambda_glue4.png)<br>   
* 	Click **Save** to save the change of function.<br><br>
* 	Now you can upload a csv file into **yourname-dataset** bucket to test that whether this Lambda function operating normally.<br><br>
* 	After you upload a csv file into **yourname-dataset** bucket, AWS Glue will run the job **“data-csv-parquet”**. The **History** console will show the job is running if your Lambda function is correct.<br><br>
![lambda_glue5.png](/images/lambda_glue5.png)<br>   
Congratulations! You now have learned how to setup an automated ETL job with Lambda function.<br>


