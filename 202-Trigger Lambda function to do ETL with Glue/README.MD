# Trigger Lambda function to do ETL with Glue

After you upload data into S3, then trigger a Lambda function to do ETL which is a type of data integration that refers to the three steps (Extrect, Transform and Load) used to blend data from multiple sources, here we will use [AWS Glue](https://aws.amazon.com/tw/glue/) to do it. [How it works?](https://docs.aws.amazon.com/glue/latest/dg/how-it-works.html)


## Prerequisites

* Make sure the region is **US East (N. Virginia)**, which its short name is **us-east-1**.


## Setup data catalog in AWS Glue

The AWS Glue Data Catalog is your persistent metadata store. It is a managed service that lets you store, annotate, and share metadata in the AWS Cloud in the same way you would in an Apache Hive metastore. Now we will create database, tables, crawlers, jobs in AWS Glue.

### Add crawler

* 	On the **Services** menu, click **AWS Glue**.

* 	Choose **Database** in the navigation pane, choose **Add database**. 

*  In the **Database name**, type **my-data**, and choose **Create**.

* 	Choose **Crawlers** in the navigation pane, choose **Add crawler**. 

*  Type Crawler name **data-crawler**, and choose **Next**.

* 	On the **Add a data store** page, choose **S3** as data store.

* 	Select **Specified path in my account**.

* 	Select the bucket that you create first (**“yourname-dataset”**), and choose **Next**.

![glue1.png](/images/glue1.png)

* 	On **Add another data store** page, choose **No**, and choose **Next**.

* 	Select **Choose an existing IAM role**, and choose the role **AWSGlueServiceRoleDefault** you just created in the drop-down list, and choose **Next**.

* 	For **Frequency**, choose **Run on demand**, and choose **Next**.

* 	For **Database**, choose **my-data**, and choose **Next**.

* 	Review the steps, and choose **Finish**.

* 	The crawler is ready to run. Choose **Run crawler**.

* 	When the crawler has finished, two table has been added. Choose **Tables** in the left navigation pane, and then choose **usvideos_csv** to confirmed.

![glue2.png](/images/glue2.png)

![glue3.png](/images/glue3.png)

Now you finish the crawler setting, and going to create a job to transform data type.


### Add jobs to transform the Data from CSV to Parquet format

* 	In the navigation pane, under **ETL**, choose **Jobs**, and then choose **Add job**.

* 	On the Job properties, enter the following details:

    * **Name: data-csv-parquet**

    * **IAM role:** choose **AWSGlueServiceRoleDefault**

* 	For **This job runs**, select **A proposed script generated by AWS Glue**.

* 	Choose **Next**.

* 	Choose **usvideos_csv** as the data source, and choose **Next**.

* 	Choose **Create tables in your data target**.

*	For Data store, choose **Amazon S3**, and choose **Parquet** as the format.

*	For **Target path**, select S3 bucket with **“yourname-etl-result”** that you created before to store the results.

![glue4.png](/images/glue4.png)

* 	Verify the schema mapping, and choose **Next** and click **Save job and edit script**. Skip Script editor tips.

![glue5.png](/images/glue5.png)

* 	View the job. This screen provides a complete view of the job and allows you to edit, click **Save**, and choose **Run job**. This steps may be waiting around 10 minutes.

![glue6.png](/images/glue6.png)

* 	When job finished, go to your S3 bucket **“yourname-etl-result”** ensure there is a parquet file that means your job succeed.

![glue7.png](/images/glue7.png)


### Add another parquet table and crawler for USvideos.csv

* 	When the job has finished, add a new table for the Parquet data using a crawler.

* 	In the navigation pane, choose **Add crawler**. Add type Crawler name **parquet-crawler1** and choose **Next**.

* 	Choose **S3** as the **Data store**.

* 	Include path choose your S3 bucket **“yourname-etl-result”** to store data.

* 	Choose **Next**.

*	On **Add another data store** page, choose **No**, and choose **Next**.

* 	Select **Choose an existing IAM role**, and choose the role **AWSGlueServiceRoleDefault** you just created in the drop-down list, and choose **Next**.

* 	For **Frequency**, choose **Run on demand**, and choose **Next**.

* 	For **Database**, choose **my-data**, and choose **Next**.

* 	Review the steps, and choose **Finish**.

* 	The crawler is ready to run. Choose **Run it now**.

* 	After the crawler has finished, there is a new table in the **my-data** database:

![glue8.png](/images/glue8.png)

* That means you finish the ETL process of csv to parquet format **(USvideos.csv)**.


### Add jobs for data of US-category-id.json

In order to analyze the channel category, we need to perform another job in Glue data catalog which contains category data (**US-category-id.json**).

* 	In the navigation pane, under **ETL**, choose **Jobs**, and then choose **Add job**.

* 	On the Job properties, enter the following details:

    * **Name: data-json-parquet**

    * **IAM role:** choose **AWSGlueServiceRoleDefault**

* 	For **This job runs**, select **A proposed script generated by AWS Glue**.

* 	Choose **Next**.

*	Choose **us-category-id_json** as the data source, and choose **Next**.

* 	Choose **Create tables in your data target**.

*	For Data store, choose **Amazon S3**, and choose **Parquet** as the format.

* 	For **Target path**, select S3 bucket with **“yourname-etl-result2”** that you created before to store the results.

![glue9.png](/images/glue9.png)

* 	Verify the schema mapping, and choose **Next** and click **Save job and edit script**.

![glue10.png](/images/glue10.png)

* 	View the job. This screen provides a complete view of the job and allows you to edit, click **Save**, and choose **Run job**. This steps may be waiting around 10 minutes.

![glue11.png](/images/glue11.png)

* 	When job finished, go to your S3 bucket **“yourname-etl-result2”** ensure there is a parquet file that means your job succeed.

![glue12.png](/images/glue12.png)  
 
Now you need to add another parquet table and crawler

* 	When the job has finished, add a new table for the Parquet data using a crawler.

* 	In the navigation pane, choose **Add crawler**. Add type Crawler name **parquet-data2** and choose **Next**.

* 	Choose **S3** as the **Data store**.

* 	Include path choose your S3 bucket **“yourname-etl-result2”** to store data.

* 	Choose **Next**.

* 	On **Add another data store** page, choose **No**, and choose **Next**.

* 	Select **Choose an existing IAM role**, and choose the role **AWSGlueServiceRoleDefault** you just created in the drop-down list, and choose **Next**.

* 	For **Frequency**, choose **Run on demand**, and choose **Next**.

* 	For **Database**, choose **my-data**, and choose **Next**.

*	Review the steps, and choose **Finish**.

* 	The crawler is ready to run. Choose **Run it now?**.

* 	After the crawler has finished, there is a new table in the **my-data** database:

![glue13.png](/images/glue13.png)

Next step you will learn how to trigger ETL job with Lambda function automatically


## Trigger ETL job with Lambda function

### Create a Lambda function in order to automate ETL job with glue (automated ETL job)

* 	On the **Services** menu, click **Lambda**.

* 	Click **Create function**.

* 	Choose **Author from scratch**.

* 	Enter function Name **ETL-auto**.

* 	Select **python 3.6** in **Runtime** blank.

* 	Select **Choose an existing role** in Role blank and choose **LambdaAutoETL** as **Existing role**.

   ![lambda_glue1.png](/images/lambda_glue1.png)

* 	Click **Create function**.

* Scroll down to the **Function code** section, then delete all of the code that appears in the code editor.

* Copy below code, and paste it into the code editor:

      import boto3
      import json

      glue = boto3.client('glue')

      def lambda_handler(event, context):
           # TODO implement
           response = glue.start_job_run(
               JobName = 'data-csv-parquet',
               Arguments = {})
           return response
 
  ![lambda_glue4.png](/images/lambda_glue4.png) 

* Scroll to **Add triggers** at top of the page.

* Under **Add triggers**, click **S3**.

* Scroll down to **Configure triggers** and use these settings:

    * Select bucket **“yourname-dataset”** as Bucket.
    
    * Select **PUT** as **Event type**.
    
    * Remember to check **Enable trigger** box.

* Click **Add**.

![lambda_glue2.png](/images/lambda_glue2.png)

![lambda_glue3.png](/images/lambda_glue3.png)

* 	Click **Save** to save the change of function.

* 	Now you can re-upload **USvideos.csv** file into **yourname-dataset** bucket again to test that whether this Lambda function operating normally.

* 	After you upload a csv file into **yourname-dataset** bucket, AWS Glue will run the job **“data-csv-parquet”**. The **History** console will show the job is running if your Lambda function is correct.

![lambda_glue5.png](/images/lambda_glue5.png)


## Congratulations! You now have learned how to:

*  Processing ETL job manually using AWS Glue and Amazon S3.

*  Crawler your data to Amazon S3 by AWS Glue.

*  Setup an automated ETL job with Lambda function.

### Now you are ready to use [AWS Comprehend to do topic modeling job](https://github.com/ecloudvalley/Serverless-ETL-and-data-analysis-on-AWS/tree/master/203-Using%20AWS%20Comprehend%20do%20topic%20modeling%20job)


